{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "matplotlib_style = 'fivethirtyeight' #@param ['fivethirtyeight', 'bmh', 'ggplot', 'seaborn', 'default', 'Solarize_Light2', 'classic', 'dark_background', 'seaborn-colorblind', 'seaborn-notebook']\n",
    "import matplotlib.pyplot as plt; plt.style.use(matplotlib_style)\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "#%matplotlib inline\n",
    "# import seaborn as sns; sns.set_context('notebook')\n",
    "from IPython.core.pylabtools import figsize\n",
    "#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)\n",
    "notebook_screen_res = 'retina' #@param ['retina', 'png', 'jpeg', 'svg', 'pdf']\n",
    "#%config InlineBackend.figure_format = notebook_screen_res\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "\n",
    "def session_options(enable_gpu_ram_resizing=True, enable_xla=False):\n",
    "    \"\"\"\n",
    "    Allowing the notebook to make use of GPUs if they're available.\n",
    "\n",
    "    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear\n",
    "    algebra that optimizes TensorFlow computations.\n",
    "    \"\"\"\n",
    "    config = tf.config\n",
    "    gpu_devices = config.experimental.list_physical_devices('GPU')\n",
    "    if enable_gpu_ram_resizing:\n",
    "        for device in gpu_devices:\n",
    "           tf.config.experimental.set_memory_growth(device, True)\n",
    "    if enable_xla:\n",
    "        config.optimizer.set_jit(True)\n",
    "    return config\n",
    "\n",
    "session_options(enable_gpu_ram_resizing=True, enable_xla=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "PATH_DATA='data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timelines to perform the model on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the model on timelines from 01-09-2019, and inputing the null dates to 0. This may cause a problem with the not fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timeline_frequency(path):\n",
    "    timeline = pd.read_pickle(PATH_DATA+'/timelines/'+path).sort_index(ascending=True).reset_index()\n",
    "    timeline.created_at = timeline.created_at.apply(lambda ts: ts-datetime.timedelta(hours=ts.hour, minutes=ts.minute, seconds=ts.second))\n",
    "    freq = timeline.created_at.value_counts(sort=False).loc[timeline.created_at.unique()]\n",
    "    freq = freq[freq.index>pd.to_datetime('2019-09-01 00:00:00+00:00')]\n",
    "    missing_dates = pd.Series(0, index=[i for i in pd.date_range(freq.index.min(), periods=(freq.index.max()-freq.index.min()).days) if i not in freq.index])\n",
    "    return pd.concat([freq, missing_dates]).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ C_i \\sim \\text{Poisson}(\\lambda)  $$\n",
    "\n",
    "\n",
    "$$\n",
    "\\lambda = \n",
    "\\begin{cases}\n",
    "\\lambda_1  & \\text{if } t \\lt \\tau \\cr\n",
    "\\lambda_2 & \\text{if } t \\ge \\tau\n",
    "\\end{cases}\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\lambda_1 \\sim \\text{Exp}( \\alpha ) \\\\\\\n",
    "&\\lambda_2 \\sim \\text{Exp}( \\alpha )\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "& \\tau \\sim \\text{DiscreteUniform(1, len(timeline) ) }\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_log_prob(count_data, lambda_1, lambda_2, tau):\n",
    "    tfd = tfp.distributions\n",
    " \n",
    "    alpha = (1. / tf.reduce_mean(count_data))\n",
    "    rv_lambda_1 = tfd.Exponential(rate=alpha)\n",
    "    rv_lambda_2 = tfd.Exponential(rate=alpha)\n",
    " \n",
    "    rv_tau = tfd.Uniform()\n",
    " \n",
    "    lambda_ = tf.gather(\n",
    "         [lambda_1, lambda_2],\n",
    "         indices=tf.cast(tau * tf.cast(tf.size(count_data), dtype=tf.float32) <= tf.cast(tf.range(tf.size(count_data)), dtype=tf.float32), dtype=tf.int32))\n",
    "    rv_observation = tfd.Poisson(rate=lambda_)\n",
    " \n",
    "    return (\n",
    "         rv_lambda_1.log_prob(lambda_1)\n",
    "         + rv_lambda_2.log_prob(lambda_2)\n",
    "         + rv_tau.log_prob(tau)\n",
    "         + tf.reduce_sum(rv_observation.log_prob(count_data))\n",
    "    )\n",
    "\n",
    "\n",
    "# Define a closure over our joint_log_prob.\n",
    "def unnormalized_log_posterior(lambda1, lambda2, tau, count_data):\n",
    "    return joint_log_prob(count_data, lambda1, lambda2, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bipoisson_model(freq, num_burnin_steps=5000, num_results=20000, step_size = 0.2):\n",
    "    count_data = tf.constant(freq.values, dtype=tf.float32)\n",
    "    # wrap the mcmc sampling call in a @tf.function to speed it up\n",
    "    @tf.function(autograph=False)\n",
    "    def graph_sample_chain(*args, **kwargs):\n",
    "        return tfp.mcmc.sample_chain(*args, **kwargs)\n",
    "    \n",
    "    # Set the chain's start state.\n",
    "    initial_chain_state = [\n",
    "        tf.cast(tf.reduce_mean(count_data), tf.float32) * tf.ones([], dtype=tf.float32, name=\"init_lambda1\"),\n",
    "        tf.cast(tf.reduce_mean(count_data), tf.float32) * tf.ones([], dtype=tf.float32, name=\"init_lambda2\"),\n",
    "        0.5 * tf.ones([], dtype=tf.float32, name=\"init_tau\"),\n",
    "    ]\n",
    "    # Since HMC operates over unconstrained space, we need to transform the\n",
    "    # samples so they live in real-space.\n",
    "    unconstraining_bijectors = [\n",
    "        tfp.bijectors.Exp(),       # Maps a positive real to R.\n",
    "        tfp.bijectors.Exp(),       # Maps a positive real to R.\n",
    "        tfp.bijectors.Sigmoid(),   # Maps [0,1] to R.  \n",
    "    ]\n",
    "    kernel = tfp.mcmc.TransformedTransitionKernel(\n",
    "        inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn=partial(unnormalized_log_posterior, count_data=count_data),\n",
    "            num_leapfrog_steps=2,\n",
    "            step_size=step_size,\n",
    "            state_gradients_are_stopped=True),\n",
    "        bijector=unconstraining_bijectors)\n",
    "    \n",
    "    kernel = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "        inner_kernel=kernel, num_adaptation_steps=int(num_burnin_steps * 0.8))\n",
    "    [lambda_1_samples, lambda_2_samples, posterior_tau], kernel_results = graph_sample_chain(\n",
    "        num_results=num_results,\n",
    "        num_burnin_steps=num_burnin_steps,\n",
    "        current_state=initial_chain_state,\n",
    "        kernel = kernel)\n",
    "    tau_samples = tf.floor(posterior_tau * tf.cast(tf.size(count_data),dtype=tf.float32))\n",
    "    #freq.index\n",
    "    return { 'tau': np.array([pd.to_datetime(freq.index.values[int(t)]) for t in tau_samples]), 'tau_samples':tau_samples, 'lambda_1':lambda_1_samples, 'lambda_2': lambda_2_samples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_save_model(i):\n",
    "    t0 = datetime.datetime.now()\n",
    "    freq = get_timeline_frequency(i)\n",
    "    model = fit_bipoisson_model(freq)\n",
    "    t1 = datetime.datetime.now()\n",
    "    model_with_freq={'model':model, 'freq':freq, 'performance':(t1-t0).seconds}\n",
    "    with open(PATH_DATA+'/models/'+i, 'wb') as file:\n",
    "        pickle.dump(model_with_freq, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return model_with_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lambdas(l1, l2, fig=None, **kwargs):\n",
    "    if not fig:        \n",
    "        fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(x=l1, histnorm='probability', name='lambda_1'), **kwargs)\n",
    "    fig.add_trace(go.Histogram(x=l2, histnorm='probability',  name='lambda_2'), **kwargs)\n",
    "#     fig.update_layout(barmode='overlay')\n",
    "#     fig.update_traces(opacity=0.75)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tau(model, fig=None, **kwargs):\n",
    "    if not fig:\n",
    "        fig = go.Figure()    \n",
    "    fig.add_trace(go.Histogram(x=model['tau'], histnorm='probability', name='tau'), **kwargs)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tau(model):\n",
    "    return pd.Series(model['tau']).value_counts()/len(model['tau'])\n",
    "def print_tau(model):\n",
    "    print(pd.Series(model['tau']).value_counts()/len(model['tau']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_texts_bipoisson(model, freq):\n",
    "    n_count_data = len(freq)\n",
    "    N_ = model['tau'].shape[0]\n",
    "    day_range = tf.range(0,n_count_data,delta=1,dtype = tf.int32)\n",
    "    day_range = tf.expand_dims(day_range,0)\n",
    "    day_range = tf.tile(day_range,tf.constant([N_,1]))\n",
    "    tau_samples_per_day = tf.expand_dims(model['tau_samples'],0)\n",
    "    tau_samples_per_day = tf.transpose(tf.tile(tau_samples_per_day,tf.constant([day_range.shape[1],1])))\n",
    "    tau_samples_per_day = tf.cast(tau_samples_per_day,dtype=tf.int32)\n",
    "    ix_day = day_range < tau_samples_per_day\n",
    "    lambda_1_samples_per_day = tf.expand_dims(model['lambda_1'],0)\n",
    "    lambda_1_samples_per_day = tf.transpose(tf.tile(lambda_1_samples_per_day,tf.constant([day_range.shape[1],1])))\n",
    "    lambda_2_samples_per_day = tf.expand_dims(model['lambda_2'],0)\n",
    "    lambda_2_samples_per_day = tf.transpose(tf.tile(lambda_2_samples_per_day,tf.constant([day_range.shape[1],1])))\n",
    "    expected_texts_per_day = ((tf.reduce_sum(lambda_1_samples_per_day*tf.cast(ix_day,dtype=tf.float32),axis=0) + tf.reduce_sum(lambda_2_samples_per_day*tf.cast(~ix_day,dtype=tf.float32),axis=0))/N_)\n",
    "    return expected_texts_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expected(model, freq, n=1, fig=None, **kwargs):\n",
    "    if not fig:\n",
    "        fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=freq.index, y=expected_texts_bipoisson(model, freq) , mode='lines', name='expected # tweets'), **kwargs)\n",
    "    fig.add_trace(go.Scatter(x=freq.index, y=freq.rolling(n).mean(), mode='lines', name='moving mean n=1'), **kwargs)\n",
    "    fig.add_trace(go.Bar(x=freq.index, y=freq.values, name='# tweets'), **kwargs)\n",
    "    return fig\n",
    "# expected_texts_per_day = tf.zeros(N_,n_count_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_everything(model, freq, n=1):\n",
    "    fig = make_subplots(rows=4, cols=1, specs=[[{}],[{}],[{\"rowspan\":2}],[None]])\n",
    "    fig.update_layout(\n",
    "        height=600)\n",
    "    plot_lambdas(model['lambda_1'],model['lambda_2'], fig=fig, row=1, col=1)\n",
    "    plot_tau(model, fig=fig, row=2, col=1)\n",
    "    return plot_expected(model, freq, n=n, fig=fig, row=3, col=1, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate bipoisson for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_some = pd.read_pickle(PATH_DATA+'/missed_some_may_update.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {}\n",
    "# for i in os.listdir('data/timelines'):\n",
    "#     if i in missed_some.values:\n",
    "#         continue\n",
    "#     try:\n",
    "#         print(i)\n",
    "#         fit_and_save_model(i)\n",
    "#     except KeyboardInterrupt:\n",
    "#         print('stopping')\n",
    "#         break\n",
    "#     except:\n",
    "#         print('bad {}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate bipoisson for given username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_profiles = pd.read_pickle(PATH_DATA+'/all_profiles.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_missed = missed_some.apply(lambda x: x[:-4]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_model = all_profiles[all_profiles.type_profile=='politician'].sample().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_freq = fit_and_save_model('{}.pkl'.format(id_to_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_everything(model_with_freq['model'], model_with_freq['freq'], n=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = all_profiles.loc[[int(i[:-4]) for i in os.listdir(PATH_DATA+'/models') if int(i[:-4]) in all_profiles.index]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles['model'] = profiles.apply(lambda x: pickle.load(open(PATH_DATA+'/models/{}.pkl'.format(x.name), \"rb\")), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_actions = profiles['model'].apply(lambda x: x['freq']).sum().reset_index().resample('W-Mon', on='index').sum()[0]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=total_actions.index, y=total_actions.values))\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_available = profiles['model'].apply(lambda x: x['freq'].apply(lambda x: 1)).sum().reset_index().resample('W-Mon', on='index').sum()[0]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=info_available.index, y=info_available.values))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_taus_profiles(profiles, fig=None, name=None, **kwargs):\n",
    "    taus = profiles['model'].apply(lambda m: get_tau(m['model']))\n",
    "    taus_sum = taus.sum(axis=0)\n",
    "    taus_weekly = taus_sum.reset_index().resample('W-Mon', on='index').sum()[0]\n",
    "    if not fig:\n",
    "        fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(x=taus_weekly.index, y=taus_weekly.values, name=name), **kwargs)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_taus_profiles(profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=profiles.type_profile.unique().shape[0], cols=1)\n",
    "for i, g in enumerate(profiles.groupby('type_profile')):\n",
    "    id_g, group = g\n",
    "    plot_taus_profiles(group, fig, name=id_g, row=i+1, col=1, )\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by follower_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=4, cols=1)\n",
    "profiles['followers_count_bins']=pd.qcut(profiles['followers_count'], q=4)\n",
    "for i, g in enumerate(profiles.groupby('followers_count_bins')):\n",
    "    id_g, group = g\n",
    "    plot_taus_profiles(group, fig, name=str(id_g), row=i+1, col=1, )\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by number of total tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=4, cols=1)\n",
    "profiles['statuses_count_bins']=pd.qcut(profiles['statuses_count'], q=4)\n",
    "for i, g in enumerate(profiles.groupby('statuses_count_bins')):\n",
    "    id_g, group = g\n",
    "    plot_taus_profiles(group, fig, name=str(id_g), row=i+1, col=1, )\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.cut(profiles['created_at'], bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles['created_at'] = profiles['created_at'].apply(lambda x: pd.to_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=4, cols=1)\n",
    "profiles['created_at_bins']=pd.qcut(profiles['created_at'], q=4)\n",
    "for i, g in enumerate(profiles.groupby('created_at_bins')):\n",
    "    id_g, group = g\n",
    "    plot_taus_profiles(group, fig, name=str(id_g), row=i+1, col=1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles['model'].apply(lambda m: m['model']['lambda_1'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 =  profiles['model'].apply(lambda m: m['model']['lambda_1'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 =  profiles['model'].apply(lambda m: m['model']['lambda_2'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_l1, max_l1 = [l1.apply(lambda m: m.min()).min(), l1.apply(lambda m: m.max()).max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_l2, max_l2 = [l2.apply(lambda m: m.min()).min(), l2.apply(lambda m: m.max()).max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1 =  profiles['model'].apply(lambda m: m['model']['lambda_1'].numpy())\n",
    "min_l, max_l = min(min_l1, min_l2), max(max_l1, max_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "x_l = list(range(math.floor(min_l), math.ceil(max_l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_l_log = np.logspace(np.log10(0.001+min_l), np.log10(math.ceil(max_l)), 50, endpoint=True)\n",
    "x_l_log = np.hstack([[min_l],x_l_log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_l1 = l1.apply(lambda m: np.histogram(m, density=True, bins=x_l_log)[0])\n",
    "df_hist_l1 = pd.DataFrame(hist_l1.to_list(), index=hist_l1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_l2 = l2.apply(lambda m: np.histogram(m, density=True, bins=x_l_log)[0])\n",
    "df_hist_l2 = pd.DataFrame(hist_l2.to_list(), index=hist_l2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_l1.values\n",
    "# hist_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_l_log = np.logspace(0.01, np.log(200), 200, endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=x_l,y=df_hist_l1.sum().values, name='lambda_1'))\n",
    "fig.add_trace(go.Bar(x=x_l,y=df_hist_l2.sum().values, name='lambda_2'))\n",
    "# fig.update_layout(barmode='overlay')\n",
    "# fig.update_traces(opacity=0.75)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff =  profiles['model'].apply(lambda m: np.mean((m['model']['lambda_1'].numpy()-m['model']['lambda_2'].numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taus_2.loc[1000016300][pd.Timestamp('2020-05-06')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taus_and_lambdas_diff = pd.DataFrame(taus.apply(lambda t: [i*mean_diff[t.name] for i in t.values], axis=1).to_list(), index=taus.index, columns=taus.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=to_plot.index, y=to_plot.values))\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = taus_and_lambdas_diff.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taus.head().apply(lambda t: mean_diff[t.name], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
